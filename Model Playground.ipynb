{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Playground\n",
    "Based on Bellotti et al.'s \"Adaptive Experience Engine for Serious Games.\"\n",
    "\n",
    "\n",
    "They proposed 2 variations of their model for serious games: a genetic algorithm model and a **reinforcement learning model** (implementation details are in p. 274-6)\n",
    "* I am interested in implementing a simple prototype based on the proposed RL model.\n",
    "* By simple, I mean I will include all of the core implementation details specified in the paper but will also exclude some other details.\n",
    "* I have started implementing some details of the genetic algorithm model, but the implementation is still _incomplete_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import sys\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Model\n",
    "Bellotti et al. specified that they used a \"Q-learning algorithm with linear function approximation.\" The following implementation of Q-learning with value function approximation is adapted from [this implementation](https://github.com/dennybritz/reinforcement-learning/blob/master/FA/Q-Learning%20with%20Value%20Function%20Approximation%20Solution.ipynb).\n",
    "\n",
    "In the following implementation,\n",
    "* a **state** refers to a selected sequence of tasks (these tasks are presented to the user in this order)\n",
    "* an **action** refers to appending a new task to the selected sequence of tasks\n",
    "* the **reward** is calculated as the \"edutainment\" value proposed by the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "* add names to tasks so that I can print and identify the sequenced tasks\n",
    "* prettier printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status/State Features (from table VI in Bellotti et al.'s paper)\n",
    "This implementation will only consider a subset of these features:\n",
    "* Average Score (AS): average score obtained by the player in the tasks in the selected sequence\n",
    "* Average Length (AL): average time-length spent by the player for tasks in the selected sequence\n",
    "* Interrupted Tasks Frequency (ITF): ratio between number of tasks the player has quit before the end and the total number of tasks in the selected sequence\n",
    "* Variety of the task types (VT): variance of the distribution of the task types in the selected sequence\n",
    "\n",
    "Features like AD, AL and ITF are supposed to be values collected from the user's gameplay, but since I am not implementing a game, I will simple simulate gameplay data collection by manually input the values of these features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example, the set of task types could include puzzles, quizzes, and conversations with virtual characters\n",
    "\n",
    "TYPES = set([\"puzzle\", \"quiz\", \"conversation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task():\n",
    "    def __init__(self, *args):\n",
    "        self.types = {t:0 for t in TYPES}\n",
    "        for t in args:\n",
    "            if t in TYPES:\n",
    "                self.types[t] = 1  # indicates that the task is of this type\n",
    "        \n",
    "        self.score = 0  # how well the user scored on this task, normalized to [0,1]\n",
    "        self.length = 0  # how long the user spent on this task, in minutes\n",
    "        self.interrupted = 0  # iwhether or not the user quit before the end of this task, int 1 or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 example tasks for testing the behavior of this model\n",
    "\n",
    "puzzle = Task(\"puzzle\")  # a puzzle-type task\n",
    "convo = Task(\"conversation\")  # a conversation-type task\n",
    "quiz = Task(\"quiz\")  # a quiz-type task\n",
    "convo_quiz = Task(\"conversation\", \"quiz\")  # a task that e.g. incorporates quizzes within conversation\n",
    "\n",
    "TASKS = set([puzzle, convo, quiz, convo_quiz])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(state):\n",
    "    # given the state, or the sequence (list) of selected tasks,\n",
    "    # this function calculates the state features as an array of AS, AL, ITF and VT values (returned in this order)\n",
    "    \n",
    "    num_tasks = len(state)\n",
    "    \n",
    "    if num_tasks == 0:\n",
    "        return [0,0,0,0]\n",
    "    \n",
    "    AS = 0  # tracks the average score in the selected sequence\n",
    "    AL = 0  # tracks the average time-length in the selected sequence\n",
    "    \n",
    "    # ITF tracks the ratio between number of tasks the player has quit before the end and \n",
    "    # the total number of tasks in the selected sequence\n",
    "    ITF = 0\n",
    "    \n",
    "    # dictionary that tracks the distribution of the task types in the selected sequence\n",
    "    type_dist = {t:0 for t in TYPES}\n",
    "    \n",
    "    for t in state:\n",
    "        AS += t.score\n",
    "        AL += t.length\n",
    "        ITF += t.interrupted\n",
    "        \n",
    "        for type in t.types:\n",
    "            type_dist[type] += t.types[type]\n",
    "    \n",
    "    AS = AS/num_tasks\n",
    "    AL = AL/num_tasks\n",
    "    ITF = ITF/num_tasks\n",
    "    VT = np.var(list(type_dist.values()))  # the variance in type_dist\n",
    "    \n",
    "    return [AS, AL, ITF, VT]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Value: \"Edutainment\"\n",
    "\n",
    "According to the paper, the **edutainment value** is calculated as follows:\n",
    "\n",
    "$$\\alpha * \\frac{TTV}{ITF} + (1-\\alpha)*AS$$\n",
    "\n",
    "\"The entertainment is proportional to the ratio between the variety of the task types in the sequence (TTV) and the frequency of interrupted tasks (ITF). The learning is evaluated as the average score (AS) obtained by the player in the sequence, assuming that the game score rewards the player’s learning [84]. Finally, the two goals are weighted through a tradeoff factor within the 0–1 range, which indicates the relative importance of the two goals.\"\n",
    "* I think the TTV value is the same as the VT feature value (variance of the distribution of the task types in the selected sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_reward(state, a=0.5):\n",
    "    # given the state, or the sequence (list) of selected tasks,\n",
    "    # this function calculates the edutainment value\n",
    "    \n",
    "    if len(state) == 0:\n",
    "        return 0\n",
    "    \n",
    "    AS, _, ITF, VT = calc_features(state)\n",
    "    if ITF == 0:\n",
    "        ITF = 1  # to avoid a infinitely large reward when we divide by 0\n",
    "    return a*VT/ITF + (1-a)*AS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator for value function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"\n",
    "    Value Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = SGDRegressor(learning_rate=\"constant\")\n",
    "        self.model.partial_fit(self.featurize_state([]), [0])\n",
    "        \n",
    "    def featurize_state(self, state):\n",
    "        \"\"\"\n",
    "        Returns the featurized representation for a state.\n",
    "        \"\"\"\n",
    "\n",
    "        return [calc_features(state)]\n",
    "    \n",
    "    def predict(self, s, a=None):\n",
    "        \"\"\"\n",
    "        Makes value function predictions.\n",
    "        \n",
    "        Args:\n",
    "            s: state to make a prediction for\n",
    "            \n",
    "        Returns single value prediction\n",
    "            \n",
    "        \n",
    "        eturns\n",
    "            If an action a is given this returns a single number as the prediction.\n",
    "            If no action is given this returns a vector or predictions for all actions\n",
    "            in the environment where pred[i] is the prediction for action i.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        if not a:\n",
    "            unselected_tasks = TASKS-set(s)\n",
    "            q_val_dict = dict()\n",
    "\n",
    "            for t in unselected_tasks:  # calculate q value for all possible actions\n",
    "                s.append(t)\n",
    "                features = self.featurize_state(s)\n",
    "                \n",
    "                # this new action/state only contributes a nonzero value for its type\n",
    "                # (the user has not yet performed the task, so score, length and interrupted values are all zero)\n",
    "                # thus, the following prediction is only dependent on the task type variation (VT) feature\n",
    "                q_val = self.model.predict(features)[0]\n",
    "                \n",
    "                q_val_dict[t] = q_val\n",
    "                s.pop()\n",
    "            \n",
    "            return q_val_dict\n",
    "        else:\n",
    "            s.append(a)\n",
    "            features = self.featurize_state(s)\n",
    "            q_val = estimator.predict(features)[0]\n",
    "            s.pop()\n",
    "            \n",
    "            return q_val\n",
    "    \n",
    "    def update(self, s, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator parameters for a given state the target y.\n",
    "        \"\"\"\n",
    "        features = self.featurize_state(s)\n",
    "        self.model.partial_fit(features, [y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, epsilon):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(state):\n",
    "        # the state is represented as the list of selected tasks\n",
    "        unselected_tasks = TASKS-set(state)\n",
    "        action_probs = {t:epsilon/len(unselected_tasks) for t in unselected_tasks}\n",
    "        \n",
    "        highest_q_val = -1\n",
    "        best_action = None  # aka best task to append to the current state (selected sequence of tasks)\n",
    "        \n",
    "        q_val_dict = estimator.predict(state)\n",
    "        for task in q_val_dict:\n",
    "            if q_val_dict[task] > highest_q_val:\n",
    "                best_action = task\n",
    "        \n",
    "        action_probs[best_action] += (1.0 - epsilon)\n",
    "        return action_probs\n",
    "    \n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(estimator, num_episodes, discount_factor=1.0, epsilon=0.1, epsilon_decay=1.0):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for fff-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        estimator: Action-Value function estimator\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "        epsilon_decay: Each episode, epsilon is decayed by this factor\n",
    "    \n",
    "    Returns:\n",
    "        An ordered list of tasks.\n",
    "    \"\"\" \n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # state is the list/sequence of selected tasks\n",
    "        state = []\n",
    "        \n",
    "        # indicator of when we are done with sequencing tasks: \n",
    "        # when the user wants to stop or when we have sequenced all tasks\n",
    "        done = False\n",
    "        \n",
    "        # The policy we're following\n",
    "        policy = make_epsilon_greedy_policy(\n",
    "            estimator, epsilon * epsilon_decay**i_episode)\n",
    "        \n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        print(i_episode)\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "                        \n",
    "            # Choose an action (unselected task) to take\n",
    "            action_probs = policy(state)  # dictionary\n",
    "            \n",
    "            action_choices = []\n",
    "            action_choices_probs = []\n",
    "            \n",
    "            for t in action_probs:\n",
    "                action_choices.append(t)\n",
    "                action_choices_probs.append(action_probs[t])\n",
    "                \n",
    "            action = np.random.choice(action_choices, p=action_choices_probs)\n",
    "            \n",
    "            # Take a step\n",
    "            next_state = state.copy()\n",
    "            next_state.append(action)  # add the best next task to the selected sequence of tasks\n",
    "            # and perform the task\n",
    "            action.score = float(input(\"Score between 0 and 1\"))\n",
    "            action.length = float(input(\"Number of minutes spent\"))\n",
    "            action.interrupted = int(input(\"Interrupted?: 1 for yes, 0 for no\"))\n",
    "            \n",
    "            done = bool(int(input(\"Want to stop doing tasks?: 1 for yes, 0 for no\")))\n",
    "            \n",
    "            reward = calc_reward(next_state)\n",
    "            \n",
    "            if len(next_state) == len(TASKS):\n",
    "                done = True\n",
    "            \n",
    "            # TD Update\n",
    "            q_values_next_dict = estimator.predict(next_state)\n",
    "            \n",
    "            # Use this code for Q-Learning\n",
    "            # Q-Value TD Target\n",
    "            td_target = reward + discount_factor * np.max(list(q_values_next_dict.values()))\n",
    "            print(reward)\n",
    "            \n",
    "            # Update the function approximator using our target\n",
    "            estimator.update(next_state, td_target)\n",
    "            \n",
    "            print(\"\\rStep {} @ Episode {}/{} ({})\".format(t, i_episode + 1, num_episodes, calc_reward(state)), end=\"\")\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ChentianJiang/miniconda3/envs/ospri/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "estimator = Estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ChentianJiang/miniconda3/envs/ospri/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3611111111111111\n",
      "Step <__main__.Task object at 0x1077916d8> @ Episode 1/3 (0)1\n",
      "0.5111111111111112\n",
      "Step <__main__.Task object at 0x1077916d8> @ Episode 2/3 (0)0.4861111111111111\n",
      "Step <__main__.Task object at 0x107791748> @ Episode 2/3 (0.5111111111111112)2\n",
      "0.2611111111111111\n",
      "Step <__main__.Task object at 0x1077916d8> @ Episode 3/3 (0)"
     ]
    }
   ],
   "source": [
    "q_learning(estimator, 3, epsilon=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Model (Incomplete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the models below, there is a difference between calling \"make\" and \"set\" for an instance.\n",
    "* calling the \"make\" function will create an instance attribute from scratch everytime\n",
    "* calling the \"set\" function will _modify_ an existing attribute everytime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the original sigmoid function\n",
    "x = np.arange(-10,10,0.1)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the transformed sigmoid function\n",
    "# that will be used for the Task model's skill benefit curve\n",
    "x = np.arange(0,10,0.1)\n",
    "y = sigmoid(-(x-5))\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predefined Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LEVELS = 10\n",
    "TYPES = set()\n",
    "SKILLS = set()\n",
    "LEARNING_STYLES = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: print messages\n",
    "# TODO: implement the difficulty adaptation range\n",
    "# Note: I have chosen to omit the \"Place\" parameter because I will not be using it\n",
    "\n",
    "class Task:\n",
    "    def make_types(self, **kwargs):\n",
    "        # key: string (task type)\n",
    "        # value: int/float (numeric extent to which this task covers the associated type)\n",
    "        # all values are normalized to add up to 1\n",
    "        total = sum(kwargs.values())\n",
    "        self.types = dict()\n",
    "        for t in kwargs:\n",
    "            if t in TYPES:\n",
    "                self.types[t] = kwargs[t]/total\n",
    "            else:\n",
    "                print(\"Type {} has not been predefined and therefore has not been added to the task.\"\\\n",
    "                      .format(t))\n",
    "            \n",
    "    def make_val(self, symbol, val, range=5):\n",
    "        # setting values such that they are normalized to 0..1\n",
    "        if symbol == 'E':\n",
    "            self.entertainment_val = val/range\n",
    "        elif symbol == 'Q':\n",
    "            self.quality_val = val/range\n",
    "        elif symbol == 'L':\n",
    "            self.length = val/range\n",
    "        elif symbol == 'IL':\n",
    "            self.interactivity_level = val/range\n",
    "        \n",
    "    def make_skill_relevance(self, **kwargs):\n",
    "        # key: string (skill name)\n",
    "        # value: int/float (numeric extent to which this task covers the associated skill)\n",
    "        # all values are normalized to add up to 1\n",
    "        total = sum(kwargs.values())\n",
    "        self.skill_relevance = dict()\n",
    "        for skill in kwargs:\n",
    "            if skill in SKILLS:\n",
    "                self.skill_relevance[skill] = kwargs[skill]/total\n",
    "            else:\n",
    "                print(\"Skill {} has not been predefined and therefore has not been added to the task.\"\\\n",
    "                      .format(skill))\n",
    "        \n",
    "    def calc_skill_benefits(self, arg):\n",
    "        # for each skill, we calculate skill benefit = sigmoid(-(x-5))\n",
    "        # x is the player's ability level in the skill\n",
    "        # the transformed sigmoid is a 0-1 normalized s-curve in the range of x=0..10\n",
    "        # for now, I will use the same skill benefit function for all skills\n",
    "        \n",
    "        if isinstance(arg, User):  # input is a user\n",
    "            if hasattr(self, 'skill_relevance'):\n",
    "                skill_benefits = dict()\n",
    "                for skill in self.skill_relevance:\n",
    "                    x = arg.skill_levels[skill]  # user's skill level is in range 0..1\n",
    "                    x = x*10  # scale the skill level to 0..10\n",
    "                    skill_benefits[skill] = sigmoid(-(x-5))\n",
    "                return skill_benefits\n",
    "            else:\n",
    "                print(\"Could not calculate skill benefits because make_skill_relevance has not been called.\")\n",
    "            \n",
    "        elif isinstance(arg, (float,int)):\n",
    "            # input is a numeric skill level for one skill\n",
    "            x = arg*(10/NUM_LEVELS)  # scale to range 0..10\n",
    "            return sigmoid(-(x-5))\n",
    "    \n",
    "    def calc_difficulty(self):\n",
    "        # estimate the difficulty of this task instance\n",
    "        # as a function of skill benefits, which are weighted by the corresponding skill relevance\n",
    "        if hasattr(self, 'skill_relevance'):\n",
    "            sum_outer = 0\n",
    "            for skill in self.skill_relevance:\n",
    "                sum_inner = 0\n",
    "                for j in range(NUM_LEVELS):\n",
    "                    sum_inner += j*self.calc_skill_benefits(j)\n",
    "                sum_outer += self.skill_relevance[skill]*sum_inner\n",
    "            return sum_outer\n",
    "        else:\n",
    "            print(\"Could not calculate difficulty because make_skill_relevance has not been called.\")\n",
    "            \n",
    "    # TODO: difficulty adaptation range\n",
    "        \n",
    "    def make_learning_styles(self, **kwargs):\n",
    "        # key: string (learning style)\n",
    "        # value: int/float (numeric extent to which this task covers the associated style)\n",
    "        # all values are normalized to add up to 1\n",
    "        total = sum(kwargs.values())\n",
    "        self.learning_styles = dict()\n",
    "        for style in kwargs:\n",
    "            if style in LEARNING_STYLES:\n",
    "                self.learning_styles[style] = kwargs[style]/total\n",
    "            else:\n",
    "                print(\"Learning style {} has not been predefined and therefore has not been added to the task.\"\\\n",
    "                      .format(style))\n",
    "        \n",
    "    def make_dependencies(self, **args):\n",
    "        self.dependencies = set()\n",
    "        for task in args:  # the current Task instance depends on these other Task instances\n",
    "            self.dependencies.add(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: I have chosen to omit the \"Navigation Ability Level\" parameter because I will not be using it\n",
    "# \"Navigation Ability Level\" is associated with the Task model's \"Place\" parameter,\n",
    "# which I will also not be using\n",
    "\n",
    "class User:\n",
    "    def __init__(self):\n",
    "        self.skill_levels = {skill:0 for skill in SKILLS}\n",
    "        \n",
    "        self.type_pref_unscaled = {t:0 for t in TASK_TYPES}\n",
    "        self.type_pref_scaled = {t:0 for t in TASK_TYPES}\n",
    "        \n",
    "        self.skill_pref_unscaled = {s:0 for s in SKILLS}\n",
    "        self.skill_pref_scaled = {s:0 for s in SKILLS}\n",
    "        \n",
    "        self.learning_style_pref_unscaled = {ls:0 for ls in LEARNING_STYLES}\n",
    "        self.learning_style_pref_scaled = {ls:0 for ls in LEARNING_STYLES}\n",
    "        \n",
    "    def set_skill_level(self, skill, level):\n",
    "        if skill in SKILLS:\n",
    "            self.skill_levels[skill] = level/NUM_LEVELS  # normalize to range 0..1\n",
    "        else:\n",
    "            print(\"Skill {} has not been predefined and therefore will not be included in the user's skill levels.\"\\\n",
    "                      .format(skill))\n",
    "            \n",
    "    def set_type_pref(self, **kwargs):\n",
    "        # key: string (task type)\n",
    "        # value: int/float (numeric preference for the associated type)\n",
    "        for t in kwargs:\n",
    "            if t in TYPES:\n",
    "                self.type_pref_unscaled[t] = kwargs[t]\n",
    "            else:\n",
    "                print(\"Type {} has not been predefined and therefore will not be included in the user's preferences.\"\\\n",
    "                      .format(t))\n",
    "        \n",
    "        # all values in type_pref_unscaled are normalized/scaled to add up to 1\n",
    "        total = sum(self.types_pref_unscaled.values())\n",
    "        for t in self.types_pref_unscaled:\n",
    "            self.type_pref_scaled[t] = self.type_pref_unscaled[t]/total\n",
    "                \n",
    "    def set_skill_pref(self, **kwargs):\n",
    "        # key: string (skill)\n",
    "        # value: int/float (numeric preference for the associated skill)\n",
    "        for skill in kwargs:\n",
    "            if t in SKILLS:\n",
    "                self.skill_pref_unscaled[skill] = kwargs[skill]\n",
    "            else:\n",
    "                print(\"Skill {} has not been predefined and therefore will not be included in the user's preferences.\"\\\n",
    "                      .format(skill))\n",
    "        \n",
    "        # all values in skill_pref_unscaled are normalized/scaled to add up to 1\n",
    "        total = sum(self.skill_pref_unscaled.values())\n",
    "        for t in self.skill_pref_unscaled:\n",
    "            self.skill_pref_scaled[skill] = self.skill_pref_unscaled[skill]/total\n",
    "            \n",
    "    def set_learning_style_pref(self, **kwargs):\n",
    "        # key: string (skill)\n",
    "        # value: int/float (numeric preference for the associated learning style)\n",
    "        for style in kwargs:\n",
    "            if style in LEARNING_STYLES:\n",
    "                self.learning_style_pref_unscaled[style] = kwargs[style]\n",
    "            else:\n",
    "                print(\"Learning style {} has not been predefined and therefore will not be included in the user's preferences.\"\\\n",
    "                      .format(style))\n",
    "        \n",
    "        # all values in learning_style_pref_unscaled are normalized/scaled to add up to 1\n",
    "        total = sum(self.learning_style_pref_unscaled.values())\n",
    "        for style in self.learning_style_pref_unscaled:\n",
    "            self.learning_style_pref_scaled[style] = self.learning_style_pref_unscaled[style]/total\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: user-task parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (ospri)",
   "language": "python",
   "name": "ospri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
